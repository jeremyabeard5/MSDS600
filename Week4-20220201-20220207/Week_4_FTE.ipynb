{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0552cb99",
   "metadata": {},
   "source": [
    "# Tree-based Machine Learning and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e730c",
   "metadata": {},
   "source": [
    "This week we are learning about tree-based ML algorithms and how to select features for ML algorithms. Tree-based ML algorithms include:\n",
    "\n",
    "- decision trees\n",
    "- random forests\n",
    "- boosting algorithms (e.g. xgboost, lightgbm, catboost)\n",
    "\n",
    "The second topic we'll look at is selecting features. Some features will be more important for predicting the target than others, and we can use a few ways to select the best features:\n",
    "\n",
    "- univariate statistics (like correlations)\n",
    "- selection with ML algorithms (forward, backward, recursive selection)\n",
    "- selection using feature importances from ML algorithms\n",
    "\n",
    "We'll start with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf1186",
   "metadata": {},
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faaffb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83a98d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/prepped_diabetes_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18824/2104314336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/prepped_diabetes_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Patient number'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/prepped_diabetes_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/prepped_diabetes_data.csv', index_col='Patient number')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a3ba6",
   "metadata": {},
   "source": [
    "Let's create features and targets again, as well as train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087aaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Diabetes', axis=1)\n",
    "targets = df['Diabetes']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, stratify=targets, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6ac63",
   "metadata": {},
   "source": [
    "Using the decision tree is similar to logistic regression in sklearn - we create the class, then use the fit method. It has the same score method (and other methods like predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e40efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "print(dt.score(x_train, y_train))\n",
    "print(dt.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a75bd",
   "metadata": {},
   "source": [
    "We can see the accuracy on the train set is perfect - 100%, but the test score is much lower at 87.7%. This is a classic sign of overfitting. We can see how deep the tree is and plot it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18bd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15, 15))\n",
    "_ = plot_tree(dt, fontsize=8, feature_names=features.columns, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746fc579",
   "metadata": {},
   "source": [
    "The plot shows us the nodes of the tree - the root node is at the top and leaf nodes are at the bottom. The color is blue for mostly class 1 and orange for mostly class 0. We can see most of the leaf nodes end up with pure samples of one or the other class, and some of the leaves only have 1 sample in them.\n",
    "\n",
    "Seeing how deep the tree got and the number of samples in the leaf nodes is helpful. We can restrict the number of levels of splits with max_depth. You can try a few values below and see how it changes. Here, we settled on 2 since that results in nearly equal train/test scores and looks like it eliminates the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2)\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "print(dt.score(x_train, y_train))\n",
    "print(dt.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8, 8))\n",
    "_ = plot_tree(dt, fontsize=10, feature_names=features.columns, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ec7be",
   "metadata": {},
   "source": [
    "## Plotting decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2cda0",
   "metadata": {},
   "source": [
    "As we already saw, we can plot decision trees with sklearn's `sklearn.tree.plot_tree` function. Most arguments are self-explanatory and also laid out in the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ccfe4",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e6a7e",
   "metadata": {},
   "source": [
    "Random forests improve upon decision trees in a few ways:\n",
    "- the algo uses many decision trees\n",
    "- each decision tree is trained on a subset of data (bootstrapped or sampling with replacement from the original data)\n",
    "- each DT uses a subset of features at each split\n",
    "\n",
    "This can simultaneously reduce the bias and variance of the overall algorithm compared with decision trees (sometimes). We saw how easily decision trees can overfit to the data (high variance), so this is a good improvement. We can use this with sklearn like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e854be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(rfc.score(x_train, y_train))\n",
    "print(rfc.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738bf61",
   "metadata": {},
   "source": [
    "Oops, looks like we are still overfitting. We can reduce the max depth a bit more to improve this. Try some different values and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b15b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(rfc.score(x_train, y_train))\n",
    "print(rfc.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c3462",
   "metadata": {},
   "source": [
    "It looks like even with a low max depth, the random forest is still not outperforming the decision tree like we might have guessed. We can tune some other hyperparameters, like max_features. The default for this is the square root of the number of features, which is around 4. We can actually improve our performance a bit by tuning the max features and it looks like 7 is a good value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=2, max_features=7, random_state=42)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(rfc.score(x_train, y_train))\n",
    "print(rfc.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c7c23",
   "metadata": {},
   "source": [
    "To automatically tune these hyperparameters like max_features and max_depth, we can use hyperparameter search like [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) or [Bayesian search](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html). See the advanced section of week 3's FTE for an example of Bayesian search, or the sklearn documentation for examples of gridsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056941d",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3766bc77",
   "metadata": {},
   "source": [
    "Feature selection is an important part of machine learning. If we don't have enough features, our ML algorithm may not perform well, but if we have too many features, the same thing can happen. The is a concept in machine learning and data science called \"[the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\". This states that as we have an increasing number of features, we need an exponentially increasing number of samples to cover the full sample space. One analogy is searching for a penny on a 100 meter line - we search down the line and find it eventually. If we change the line into a box (100m x 100m), then we have a much larger area to search. If we change this into a cube (100m ^3) then we have an exponentially increasing sample space to search. These dimensions are like our features.\n",
    "\n",
    "A few ways to solve the curse of dimensionality problem are to get more data or remove some of our features. We can also reduce dimensions with dimensionality reduction techniques like principle component analysis (PCA) or singular value decomposition (SVD).\n",
    "\n",
    "Here, we will focus on removing some features. The first way we'll look at are univariate statistics. These are stats between features and the target, such as correlation.\n",
    "\n",
    "Second, we'll look at using feature importances from machine learning methods.\n",
    "\n",
    "In the advanced section, we'll look at other methods with ML algos.\n",
    "\n",
    "Let's start with the univariate stats method with Pearson correlation. \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede504c1",
   "metadata": {},
   "source": [
    "This is called a correlogram, and plots the correlation of our dataframe matrix. By default, it uses the Pearson correlation, but there are some other correlations such as Kendall's Tau and Spearman. The Phik correlation has also been mentioned in week 1, and is in the advanced section here.\n",
    "\n",
    "The `annot=True` argument shows the values in each square, making it a little easier to read and interpret.\n",
    "\n",
    "Values close to 0 have no correlation, while +1 means perfect positive linear correlation and -1 means perfect inverse correlation (one variable goes up while the other goes down).\n",
    "\n",
    "We can see from the correlogram that diastolic BP, height, and gender don't seem to be linear correlated to the target. However, there can be non-linear relationships too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89be6ee",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6737f",
   "metadata": {},
   "source": [
    "For non-linear relationships between the feature and target, we can use feature importances from tree-based methods like decision trees and random forests. This method calculates the improvement in the Gini index or Entropy for each feature and ranks the features in order of which ones separate the classes best (see the slide deck for the week and/or [this article](https://quantdare.com/decision-trees-gini-vs-entropy/) for more on Gini/Entropy).\n",
    "\n",
    "We can plot feature importances the hard way (such as [this](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)), or we can use a [pre-built function](https://scikit-plot.readthedocs.io/en/stable/estimators.html#scikitplot.estimators.plot_feature_importances) from scikit-plot. First, you will need to install sckit-plot with `conda install -c conda-forge scikit-plot -y` or `pip install scikit-plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48358771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.estimators import plot_feature_importances\n",
    "plot_feature_importances(rfc, feature_names=features.columns, x_tick_rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cb6d7",
   "metadata": {},
   "source": [
    "We can see here that Glucose seems to be the most important variable, with a sharp dropoff in importance after that. We can remove some of the less-important features and see how that changes performance and the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = features.drop(['waist_hip_ratio', 'Diastolic BP', 'Weight', 'Height', 'Gender'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_features, targets, stratify=targets, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=2, max_features=7, random_state=42)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(rfc.score(x_train, y_train))\n",
    "print(rfc.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12536a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(rfc, feature_names=new_features.columns, x_tick_rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72b3e0",
   "metadata": {},
   "source": [
    "It doesn't look like our performance changed much at all, but our feature importances did. However, one thing is clear - the glucose measurement seems to be very important for predicting diabetes risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae137d0",
   "metadata": {},
   "source": [
    "These feature importance methods are good as EDA as well. If we have a target variable or want to see the relationship between variables that may be non-linear, decision tree or random forest feature importances are a great option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2dd90",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# (Optional) Advanced Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6961d5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- H2O random forests and feature importances\n",
    "- forward/backward feature selection\n",
    "- recursive feature selection\n",
    "- phik correlations\n",
    "- other univariate statistics\n",
    "\n",
    "We will demo H2O and phik here. For other feature selection methods, the [sklearn documentation](https://scikit-learn.org/stable/modules/feature_selection.html) has examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f38720",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## H2O random forests\n",
    "H2O is another machine learning library available in Python and R. It can scale up to clusters for big data. Luckly, it's easily installed with conda: `conda install -c conda-forge h2o-py -y`. When installing with pip, you also need to install a particular version of Java and potentially some other configuration issue. Conda makes it easy.\n",
    "\n",
    "Once we have it installed, we import it and initialize it. The initialization starts up the backend of H2O (which is running on Java)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb08bc9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5cf150",
   "metadata": {
    "hidden": true
   },
   "source": [
    "H2O has some impressive advantages over sklearn:\n",
    "- it can handle missing values and categorical values without modification (sklearn needs all numeric values and no missing values)\n",
    "- it can scale to big data easily (although `sklearn` can scale with the `dask` package)\n",
    "- it has some other conveniences not built-in to sklearn, like plotting feature importances\n",
    "- it has automatic ML capabilities\n",
    "\n",
    "However, the H2O Python documentation is not nearly as good as sklearn.\n",
    "\n",
    "To use H2O, we have to use a different data storage - an h2oframe. We will load our original, unmodified data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34476f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hf = h2o.H2OFrame(pd.read_excel('data/diabetes_data.xlsx', index_col='Patient number'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01568f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf9a9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hf.types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04914a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then, we can use H2O to fit a random forest. There are examples in the documentation on how to do this: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html\n",
    "\n",
    "as well as an example for tuning hyperparameters: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d75b6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "\n",
    "predictors = hf.columns\n",
    "predictors.remove('Diabetes')\n",
    "response = 'Diabetes'\n",
    "\n",
    "# Split the dataset into a train and valid set:\n",
    "train, valid = hf.split_frame(ratios=[.8], seed=1234, )\n",
    "\n",
    "# Build and train the model:\n",
    "drf = H2ORandomForestEstimator(ntrees=50,\n",
    "                                    max_depth=2,\n",
    "                                    calibrate_model=True,\n",
    "                                    calibration_frame=valid)\n",
    "drf.train(x=predictors,\n",
    "           y=response,\n",
    "           training_frame=train,\n",
    "           validation_frame=valid)\n",
    "\n",
    "# Eval performance:\n",
    "perf = drf.model_performance(valid=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f23894",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The performance metrics are easy to access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2ab06",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c27071",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting the variable (feature) importance is a built-in function. We need to set the num_of_features to our actual number of features to show them all if it's greater than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff31ace",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drf.varimp_plot(num_of_features=features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378aa4cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Phi-k correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70b96d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The phi-k correlation is a newer one (invented around 2018), and can be read more about in the [docs](https://phik.readthedocs.io/en/latest/) and their publication linked there. Essentially, it bins numeric data into categories and then uses the chi-2 correlation. We can install it with `conda install -c conda-forge phik -y` then import it. Then we have the `phik_matrix()` method for dataframes available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a2bee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import phik\n",
    "\n",
    "df.phik_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28794fc7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also create a correlogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c3eb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.phik_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763819f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The values range from 0 (no correlation) to 1 (perfect correlation), but we don't get information on the direction of the correlation. We do get correlation measurements for non-linear relationships, though. Here, we can see that gender and height seem to have the lowest correlation strengths (near 0), which agrees with the random forest importances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
